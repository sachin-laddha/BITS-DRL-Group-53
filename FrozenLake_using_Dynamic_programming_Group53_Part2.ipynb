{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group No: 53\n",
    "\n",
    "## Group Member Names:\n",
    "| Name          | BITS ID       | Contribution |\n",
    "|---------------|---------------|--------------|\n",
    "| Mujtaba Rasool| 2023ac05819   | 100%         |\n",
    "| Kartik Batta  | 2023ac05422   | 100%         |\n",
    "| Sachin Laddha | 2023ac05564   | 100%         |"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.**Problem statement**: \n",
    "\n",
    "* Develop a reinforcement learning agent using dynamic programming to solve the Treasure Hunt problem in a FrozenLake environment. The agent must learn the optimal policy for navigating the lake while avoiding holes and maximizing its treasure collection.\n",
    "\n",
    "2.**Scenario**:\n",
    "* A treasure hunter is navigating a slippery 5x5 FrozenLake grid. The objective is to navigate through the lake collecting treasures while avoiding holes and ultimately reaching the exit (goal).\n",
    "Grid positions on a 5x5 map with tiles labeled as S, F, H, G, T. The state includes the current position of the agent and whether treasures have been collected. \n",
    "\n",
    "\n",
    "#### Objective\n",
    "* The agent must learn the optimal policy π* using dynamic programming to maximize its cumulative reward while navigating the lake.\n",
    "\n",
    "#### About the environment\n",
    "\n",
    "The environment consists of several types of tiles:\n",
    "* Start (S): The initial position of the agent, safe to step.\n",
    "* Frozen Tiles (F): Frozen surface, safe to step.\n",
    "* Hole (H): Falling into a hole ends the game immediately (die, end).\n",
    "* Goal (G): Exit point; reaching here ends the game successfully (safe, end).\n",
    "* Treasure Tiles (T): Added to the environment. Stepping on these tiles awards +5 reward but does not end the game. \n",
    "\n",
    "After stepping on a treasure tile, it becomes a frozen tile (F).\n",
    "The agent earns rewards as follows:\n",
    "* Reaching the goal (G): +10 reward.\n",
    "* Falling into a hole (H): -10 reward.\n",
    "* Collecting a treasure (T): +5 reward.\n",
    "* Stepping on a frozen tile (F): 0 reward.\n",
    "\n",
    "#### States\n",
    "* Current position of the agent (row, column).\n",
    "* A boolean flag (or equivalent) for whether each treasure has been collected.\n",
    "\n",
    "#### Actions\n",
    "* Four possible moves: up, down, left, right\n",
    "\n",
    "#### Rewards\n",
    "* Goal (G): +10.\n",
    "* Treasure (T): +5 per treasure.\n",
    "* Hole (H): -10.\n",
    "* Frozen tiles (F): 0.\n",
    "\n",
    "#### Environment\n",
    "Modify the FrozenLake environment in OpenAI Gym to include treasures (T) at certain positions. Inherit the original FrozenLakeEnv and modify the reset and step methods accordingly.\n",
    "Example grid:\n",
    "\n",
    "![image.png](attachment:image.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Outcomes:**\n",
    "1.\tCreate the custom environment by modifying the existing “FrozenLakeNotSlippery-v0” in OpenAI Gym and Implement the dynamic programming using value iteration and policy improvement to learn the optimal policy for the Treasure Hunt problem.\n",
    "2.\tCalculate the state-value function (V*) for each state on the map after learning the optimal policy.\n",
    "3.\tCompare the agent’s performance with and without treasures, discussing the trade-offs in reward maximization.\n",
    "4.\tVisualize the agent’s direction on the map using the learned policy.\n",
    "5.\tCalculate expected total reward over multiple episodes to evaluate performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required libraries and Define the custom environment - 2 Marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in /Users/sachinladdha/anaconda3/lib/python3.12/site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy in /Users/sachinladdha/anaconda3/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/sachinladdha/anaconda3/lib/python3.12/site-packages (from gym) (3.0.0)\n",
      "Requirement already satisfied: gym_notices>=0.0.4 in /Users/sachinladdha/anaconda3/lib/python3.12/site-packages (from gym) (0.0.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gym numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "import gym\n",
    "from gym.envs.toy_text import FrozenLakeEnv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom environment to create the given grid and respective functions that are required for the problem\n",
    "\n",
    "#Include functions to take an action, get reward, to check if episode is over\n",
    "class FrozenLakeTreasureEnv(FrozenLakeEnv):\n",
    "    def __init__(self):\n",
    "        super().__init__(desc=np.asarray([\n",
    "            [b'S', b'F', b'F', b'H', b'T'],\n",
    "            [b'F', b'H', b'F', b'F', b'F'],\n",
    "            [b'F', b'F', b'F', b'T', b'F'],\n",
    "            [b'T', b'F', b'H', b'F', b'F'],\n",
    "            [b'F', b'F', b'F', b'F', b'G']\n",
    "        ], dtype='|S1'), is_slippery=False)\n",
    "\n",
    "        self.treasure_locations = [(0, 4), (2, 3), (3, 0)]\n",
    "        self.treasures_collected = set()\n",
    "\n",
    "        self.nS = self.observation_space.n\n",
    "        self.nA = self.action_space.n\n",
    "        self.ncol = 5\n",
    "\n",
    "    def reset(self):\n",
    "        self.s = 0\n",
    "        self.treasures_collected = set()\n",
    "        return self.s\n",
    "\n",
    "    def step(self, a):\n",
    "        self.lastaction = a\n",
    "        curr_pos = (self.s // self.ncol, self.s % self.ncol)\n",
    "        next_state = self._move(self.s, a)\n",
    "        next_pos = (next_state // self.ncol, next_state % self.ncol)\n",
    "\n",
    "        reward = 0\n",
    "        done = False\n",
    "        next_tile = self.desc[next_pos[0]][next_pos[1]]\n",
    "        if next_tile == b'G':\n",
    "            reward = 10\n",
    "            done = True\n",
    "        elif next_tile == b'H':\n",
    "            reward = -10\n",
    "            done = True\n",
    "        elif next_tile == b'F':\n",
    "            reward = 0\n",
    "\n",
    "        if next_pos in self.treasure_locations and next_pos not in self.treasures_collected:\n",
    "            reward += 5\n",
    "            self.treasures_collected.add(next_pos)\n",
    "            self._update_desc()\n",
    "\n",
    "        self.s = next_state\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def _move(self, state, action):\n",
    "        row = state // self.ncol\n",
    "        col = state % self.ncol\n",
    "\n",
    "        if action == 0:  # left\n",
    "            col = max(col - 1, 0)\n",
    "        elif action == 1:  # down\n",
    "            row = min(row + 1, self.ncol - 1)\n",
    "        elif action == 2:  # right\n",
    "            col = min(col + 1, self.ncol - 1)\n",
    "        elif action == 3:  # up\n",
    "            row = max(row - 1, 0)\n",
    "\n",
    "        return row * self.ncol + col\n",
    "\n",
    "    def _update_desc(self):\n",
    "        desc = self.desc.tolist()\n",
    "        for treasure in self.treasures_collected:\n",
    "            row, col = treasure\n",
    "            desc[row][col] = b'F'\n",
    "        self.desc = np.asarray(desc, dtype='|S1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration Algorithm - 1 Mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, gamma=0.9, theta=1e-8):\n",
    "    V = np.zeros(env.observation_space.n)\n",
    "    policy = np.zeros(env.observation_space.n, dtype=int)\n",
    "\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(env.observation_space.n):\n",
    "            v = V[s]\n",
    "            action_values = np.zeros(env.action_space.n)\n",
    "            actions = env.P[s]  # Use environment's transition probabilities\n",
    "            for a in range(env.action_space.n):\n",
    "                action_value = 0\n",
    "                for prob, next_state, reward, done in actions[a]:\n",
    "                    row, col = next_state // env.ncol, next_state % env.ncol\n",
    "                    tile = env.desc[row][col]\n",
    "\n",
    "                    if tile == b'G':\n",
    "                        reward = 10\n",
    "                    elif tile == b'H':\n",
    "                        reward = -10\n",
    "                    elif tile == b'T':\n",
    "                        reward = 5\n",
    "                    else:  # Frozen tile\n",
    "                        reward = 0\n",
    "\n",
    "                    action_value += prob * (reward + gamma * V[next_state])\n",
    "                action_values[a] = action_value\n",
    "\n",
    "            best_action = np.argmax(action_values)\n",
    "            V[s] = action_values[best_action]\n",
    "            policy[s] = best_action\n",
    "            delta = max(delta, np.abs(v - V[s]))\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Improvement Function - 1 Mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the Optimal Value Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_value_function(V, env):\n",
    "    print(\"\\nOptimal Value Function:\")\n",
    "    map_size = int(np.sqrt(env.observation_space.n))\n",
    "    for i in range(map_size):\n",
    "        for j in range(map_size):\n",
    "            print(\"{:6.2f}\".format(V[i * map_size + j]), end=\" \")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of the learned optimal policy - 1 Mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_policy(policy, map_size):\n",
    "    actions = [\"←\", \"↓\", \"→\", \"↑\"]\n",
    "    policy_grid = policy.reshape(map_size, map_size)\n",
    "    print(\"\\nOptimal Policy:\")\n",
    "    for row in policy_grid:\n",
    "        print(\" \".join([actions[action] for action in row]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the policy - 1 Mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(env, policy, num_episodes=100):\n",
    "    total_rewards = []\n",
    "    for _ in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        if isinstance(state, tuple):\n",
    "            state = state[0]\n",
    "\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = policy[int(state)]\n",
    "            step_result = env.step(action)\n",
    "\n",
    "            if len(step_result) == 5:\n",
    "                next_state, reward, terminated, truncated, info = step_result\n",
    "                done = terminated or truncated\n",
    "            else:\n",
    "                next_state, reward, done, info = step_result\n",
    "\n",
    "            if isinstance(next_state, tuple):\n",
    "                next_state = next_state[0]\n",
    "\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "        total_rewards.append(total_reward)\n",
    "    return np.mean(total_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Environment With Treasures ===\n",
      "\n",
      "Optimal Value Function:\n",
      " 51.88  56.79  63.10 -100.00  72.90 \n",
      " 57.64 -100.00  70.11  77.90  81.00 \n",
      " 64.05  70.11  77.90  81.00  90.00 \n",
      " 65.61  72.90 -100.00  90.00 100.00 \n",
      " 72.90  81.00  90.00 100.00 100.00 \n",
      "\n",
      "Optimal Policy:\n",
      "↓ → ↓ ← ↓\n",
      "↓ ← ↓ ↓ ↓\n",
      "↓ → → ↓ ↓\n",
      "↓ ↓ ← ↓ ↓\n",
      "→ → → → ←\n",
      "\n",
      "Average Reward with Treasures: 15.0\n",
      "\n",
      "=== Environment Without Treasures ===\n",
      "\n",
      "Optimal Value Function:\n",
      " 47.83  53.14  59.05 -100.00  72.90 \n",
      " 53.14 -100.00  65.61  72.90  81.00 \n",
      " 59.05  65.61  72.90  81.00  90.00 \n",
      " 65.61  72.90 -100.00  90.00 100.00 \n",
      " 72.90  81.00  90.00 100.00 100.00 \n",
      "\n",
      "Optimal Policy:\n",
      "↓ → ↓ ← ↓\n",
      "↓ ← ↓ ↓ ↓\n",
      "↓ ↓ → ↓ ↓\n",
      "↓ ↓ ← ↓ ↓\n",
      "→ → → → ←\n",
      "\n",
      "Average Reward without Treasures: 1.0\n"
     ]
    }
   ],
   "source": [
    "    # Create environment with treasures\n",
    "    env = FrozenLakeTreasureEnv()\n",
    "\n",
    "    # Run value iteration\n",
    "    V, policy = value_iteration(env)\n",
    "\n",
    "    # Print results for environment with treasures\n",
    "    print(\"\\n=== Environment With Treasures ===\")\n",
    "    print_value_function(V, env)\n",
    "    visualize_policy(policy, 5)\n",
    "    reward_with_treasures = evaluate_policy(env, policy)\n",
    "    print(\"\\nAverage Reward with Treasures:\", reward_with_treasures)\n",
    "\n",
    "    # Create environment without treasures\n",
    "    env_no_treasures = FrozenLakeEnv(desc=np.asarray([\n",
    "        [b'S', b'F', b'F', b'H', b'F'],\n",
    "        [b'F', b'H', b'F', b'F', b'F'],\n",
    "        [b'F', b'F', b'F', b'F', b'F'],\n",
    "        [b'F', b'F', b'H', b'F', b'F'],\n",
    "        [b'F', b'F', b'F', b'F', b'G']\n",
    "    ], dtype='|S1'), is_slippery=False)\n",
    "\n",
    "    # Run value iteration for environment without treasures\n",
    "    V_no_treasures, policy_no_treasures = value_iteration(env_no_treasures)\n",
    "\n",
    "    # Print results for environment without treasures\n",
    "    print(\"\\n=== Environment Without Treasures ===\")\n",
    "    print_value_function(V_no_treasures, env_no_treasures)\n",
    "    visualize_policy(policy_no_treasures, 5)\n",
    "    reward_no_treasures = evaluate_policy(env_no_treasures, policy_no_treasures)\n",
    "    print(\"\\nAverage Reward without Treasures:\", reward_no_treasures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
