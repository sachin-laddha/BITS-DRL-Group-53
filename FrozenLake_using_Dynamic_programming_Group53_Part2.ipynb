{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Group No: 53\n",
    "\n",
    "## Group Member Names:\n",
    "| Name          | BITS ID       | Contribution |\n",
    "|---------------|---------------|--------------|\n",
    "| Mujtaba       |               | 100%         |\n",
    "| Kartik Batta  |               | 100%         |\n",
    "| Sachin Laddha | 2023ac05564   | 100%         |"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfkAAAC1CAYAAABcW4ZHAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAAFiUAABYlAUlSJPAAAA3FSURBVHhe7d1fjFzVfQfwQ9XIDjhBjm1oKkPy4DoSsiubrYtForrYIiUSRhGSK/wAjihY5iGWiBv1AaHIQkitZIwKRQgsrfjzgPBDFFlG6R9iyw9xeTGEqlGEtYgCdivXIjROaVjTf3PunFmv7bVn596Ze9ZnPx/paH537moffnvt796555y96v86AgBQnN9KrwBAYYQ8ABRKyANAoYQ8ABRKyANAoYQ8ABRKyANAoYQ8ABRKyANAoYQ8ABRKyANAoYQ8ABRKyANAoQb+K3RXXXVVqgCANg36h2Nrhfzhw4fTEW277bbbwk/f+VU6om1f/9q1+p9R7L+/jZ1PvMXzx8nziffYg4a8j+sBoFBCHgAKJeQBoFBCHgAKJeQBoFBCHgAKJeQBoFBZQv7UqVOpAgBGpbWQ//DDD8P4+Hi1mcs999xTvT755JPh3XffTV/BqHzw3kS1icilxscfnU5fySjof15vd0bcxCWOf49vXKDfeerbt6+7gUu/Eb+O0Wgl5GPA33fffeHll19O73QdOHAgPPDAA+mIUTl96mSqZnZ2cjJVjIL+5/VReo0+Ta/T9TtPfWfOpKKP2X4dg2tlW9t4xx4DPXrppZfCDTfcED755JPwzjvvhCNHjoSHH364Okd/dba1PfbGkbBz211VvXvveFi8ZGlV94yt35Aq+ol33vqfT+z/oLuqHuqMTd0yvN8ZN3bLKf3Oc07npnugbW2PHw/hxIl00LEpNXrHjhC2bOnW0fLlIaxcmQ64pPipx5zc1rYX8A899FAV8NE111wTbr75ZgHfstVrb6lCZfqgPfrPfBKDe+PGc6NnxYrz3xfwo9PqxLtnn302fPzxx+kIABilVkI+3sH33H333eHQoUPCHmjd0c6IH89PH291BpSqlZC//fbbw6233pqOQnjssceqsN+/f396h7Yc2P9CeGX86fMG7dH/vLZ2RnwsPH38eWdAqVr9e/LxDj4G/HQx/B9//PF0RD9NJ97NxN9Hn72mE+9mov+z13TiXT8m3l3eoBPvLhQnjkV79oSwa1e3Zvbm7MS7no0bN4aDBw+GJ554Ir0TwtGjR8Obb76Zjhi1OLv7qRcPTI3nXn09naEN+p9XDPH4X+T08ZPOgFK1GvJRb1b9M888k94JYWJiIlWM2oWzu1etWZfO0Ab9B9rUesj3LFmyJFUAwCiMPOTjPvWPPPJI9ZF83AAniq+vvfZaVUerVq1KFQAwLK3cycfn7rt27Qp33nlnNXEsvva2uL333nvDTTfdVNUAwPCMPOSvv/76aqLd9CV0PY8++mi4//770xGjsmDh1akiB/3Pq1/3/XTas3lzKmhNq0vo4gY4Z8+ereoY/gyuzhI6hqfOEjqGp84SOoan6RI6mpnzS+gWL15chbuAB4DRyza7HgAYLSEPAIUS8gBQKCEPAIWqNbseAGjfoLPra4W8JUT5WEKUV/wV1/WfjyWMeel/XtX//3N5CR0A0B4hDwCFEvIAUCghDwCFEvIAUCghDwCFEvIAUCghDwCFamUznA/emwhb7xhLRxc7eHQiLF6yLB1xOXU2w3m7M9Z0y3CqM67rllP6neecOpvhuP6Hp85mLPo/PPqf15zdDOf0qZOpmtnZyclUMQofpdfo0/Q6Xb/zNOP6z0v/89L/vFq5kz/2xpGwc9tdVb1773jnt7alVd0ztn5Dquinzp38oc7Y1C3D+51xY7ec0u8859S5k3f9D0+dO0n9Hx79z2vO3slPt3rtLdUPdfqA+cL1n5f+56X/7TPxDgAKJeTnmaOdET+enz7e6gwAytP6M/nvPPT9sOiL11Z1z9b7v5sq+mn6TL4fz+Qvr+kzedd/M02fCet/M/qfV51n8q2H/EwG/X7zmZDPq2nIz8T1P3tNQ2Ym+j97+p/XFRHyF86uXLDw6rBqzbp0RD9NQ97s+maGPbve9T+YpiGj/83of151Qj777Ho/YOYT139e+p+X/rfPxDsAKJSQB4BCCXkAKFQrIR8nV5BPv+776YyW6z8v/c9L//NqZXY9w1Nndj3DU2d2PcNTZ3Y3w6P/eV0Rs+sBgHYIeQAolJAHgEIJeQAolJAHgELVml0PALRv0Nn1ltBdYSxhyUv/8+ouIUoHtC7e47n+87GEDgCYIuQBoFBCHgAKJeQBoFBCHgAKJeQBoFBCHgAKJeQBoFCtbIbzwXsTYesdY+noYgePToTFS5alIy6nzmYs+j88+p/XoJvh7NsXwvbt6eAynn8+hAcfTAdcUp3NcFz/wzNnN8M5fepkqmZ2dnIyVYyC/uel//mcOZOKPmb7dQzO9Z9XK3fyx944EnZuu6uqd+8d7/zWtrSqe8bWb0gV/dS5k9T/4dH/vAa9kz9+PIQTJ9JBx6ZN3dcdO0LYsqVbR8uXh7ByZTrgkurcybv+h6fOnXzrIf/Dwz8P1/9u518UtTQNGf1vRv/zGjTkLxRDKtqzJ4Rdu7o1s9c05F3/zdQJeRPvAKBQQh4ACtV6yB/Y/0J4Zfzp8wbt0f+89J/5zPXfvtafyc9k0O83nzV9JjwT/Z89/c/LM/m8mj6Tn4nrf/bqPJPPPrt+wcKrw6o169IR/TQNGf1vRv/zEvJ5NQ15138zdUK+9Y/rV6+9pVoy0Rt+wO3S/7z0n/nM9d8+E+8AoFBCHgAKJeQBoFCthHycXEE++p+X/s8dmzengta4/vNqZXY9w1NndjfDo/95NZ1dTzN1ZtczPFfE7HoAoB1CHgAKJeQBoFBCHgAKJeQBoFC1ZtcDAO0bdHZ9rZC3hCUfS1jyqpawpJr2xVsM138+lpDmZQkdADBFyANAoYQ8ABRKyANAoYQ8ABRKyANAoYQ8ABRKyANAoUa+Gc6+fSFs354OLuP550N48MF0wCXV2Qzng/cmwtY7xtLRxQ4enQiLlyxLR1xOnc1w3u6MNd0ynOqM67rllH7nOafOZjiu/+GpsxmO/g/PnNwM58yZVPQx269jcKdPnUzVzM5OTqaKUfgovUafptfp+p2nGdd/Xvqf18jv5I8fD+HEiXTQsWlT93XHjhC2bOnW0fLlIaxcmQ64pDp38sfeOBJ2brurqnfvHe/81ry0qnvG1m9IFf3UuZM/1Bnpsg/vd8aN3XJKv/OcU+dO3vU/PHXu5PV/eObknXwM7o0bz42eFSvOf1/At2P12luqf1TTB8wXrv+89L99Jt4BQKGEPLToaGfEj+enj7c6A2AUWv9Ts/GZcrRnTwi7dnVrZq/pM/nvPPT9sOiL11Z1z9b7v5sq+mn6TL4fz+Qvr+kzedd/M02fyet/M3WeyQv5K0zTkJ/JoN9vPhPyeTUN+Zm4/mevacjPRP9nr07I+7h+nomzW5968cDUeO7V19MZ2hBDPP4TnT5+0hm0w/Wfl/63T8jPMxfObl21Zl06A+Vz/eel/+0T8gBQKCEPAIUS8gBQqNZDfvPmVNCaBQuvThU59Ou+n85ouf7z0v+8Wl9CRzN1ltAxPHWW0DE8dZbQMTx1ltAxPJbQAQBThDwAFErIA0ChhDwAFErIA0Chas2uBwDaN+js+lohbwlFPpaw5KX/eel/XrH/T/3s2+mItu1c8yNL6ACALiEPAIUS8gBQKCEPAIUS8gBQKCEPAIUS8gBQKCEPQOt+85+fhV//cjIdMSqtbIbzwXsTYesdY+noYgePToTFS5alIy6nzmYg+j88+p+X/ufVdDOck8d/FX72D/8a/m7fO+mdrj958Gvh99YtDSv/0M/hcubsZjinT51M1czOTvptbpT0Py/9z0v/54bXnvlF+Ks/PXxRwEfxvb/Z/tNw7G9PpHcYllbu5I+9cSTs3HZXVe/eO975rXlpVfeMrd+QKvqpcyej/8Oj/3npf1517+RjwPfCfdWG3wnf/LOV4dplC6vjz87+b/jwF/8Rjv34RFgxtjRsvG9F9T4XuyK2tV299pbqH9X0QXv0Py/9z0v/2xc/ou8FfPxYfvtfrw9f/f0vhcVfvroa131lURi7Y3n1/ro7b6i+juEx8Q6AkYnP4HtuvfsrqZrZF760IFUMi5AHYGR6d/Ff3/LV6s6ddrUe8gf2vxBeGX/6vEF79D8v/c9L/9v18b/9V6pCWHbDolSFaulcfL584fiXf/pl+gqGpfWJdzMZ9PvNZ00nHs1E/2dP//PS/7wGnXgXQ/4H3/r7qv7291ZNTaqb/v5007+Gi8VfhOb8xLs4u/WpFw9MjedefT2doQ36n5f+56X/7Vr4hc+lKoTf/PqzVIXqY/vdP/5mNb730h+ldxmF7LPrV61Zl87QBv3PS//z0v92fX7R56olc1F8Nh93uevpza7vLaVjNEy8A2Bkxr61PFUh/OMP308VbRHyAIzMTd+4fupu/kd7/7naGCeunY939XGc/vCT6hyjIeQBGJn4kf3WH6w972P7uL3tX3zjtWrE7Wx7Fi767VQxLK2E/IKF1kbmpP956X9e+p9f3OTm3sfHwra//IOpsJ8uvhfPrf7jL6d3GJZWltAxPHWWEDE8+p+X/uc16BK6S4nr5P978n+qOs7Aj3f79HdFLKEDYH6Ld/a92fUCfrSEPAAUSsgDQKGEPAAUSsgDQKFqza4HANo36Oz6gUMeALgy+LgeAAol5AGgUEIeAAol5AGgUEIeAAol5AGgUEIeAAol5AGgUEIeAAol5AGgUEIeAAol5AGgUEIeAAol5AGgUEIeAIoUwv8D83EmcRnUZ5sAAAAASUVORK5CYII="
    }
   },
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1.**Problem statement**: \n",
    "\n",
    "* Develop a reinforcement learning agent using dynamic programming to solve the Treasure Hunt problem in a FrozenLake environment. The agent must learn the optimal policy for navigating the lake while avoiding holes and maximizing its treasure collection.\n",
    "\n",
    "2.**Scenario**:\n",
    "* A treasure hunter is navigating a slippery 5x5 FrozenLake grid. The objective is to navigate through the lake collecting treasures while avoiding holes and ultimately reaching the exit (goal).\n",
    "Grid positions on a 5x5 map with tiles labeled as S, F, H, G, T. The state includes the current position of the agent and whether treasures have been collected. \n",
    "\n",
    "\n",
    "#### Objective\n",
    "* The agent must learn the optimal policy π* using dynamic programming to maximize its cumulative reward while navigating the lake.\n",
    "\n",
    "#### About the environment\n",
    "\n",
    "The environment consists of several types of tiles:\n",
    "* Start (S): The initial position of the agent, safe to step.\n",
    "* Frozen Tiles (F): Frozen surface, safe to step.\n",
    "* Hole (H): Falling into a hole ends the game immediately (die, end).\n",
    "* Goal (G): Exit point; reaching here ends the game successfully (safe, end).\n",
    "* Treasure Tiles (T): Added to the environment. Stepping on these tiles awards +5 reward but does not end the game. \n",
    "\n",
    "After stepping on a treasure tile, it becomes a frozen tile (F).\n",
    "The agent earns rewards as follows:\n",
    "* Reaching the goal (G): +10 reward.\n",
    "* Falling into a hole (H): -10 reward.\n",
    "* Collecting a treasure (T): +5 reward.\n",
    "* Stepping on a frozen tile (F): 0 reward.\n",
    "\n",
    "#### States\n",
    "* Current position of the agent (row, column).\n",
    "* A boolean flag (or equivalent) for whether each treasure has been collected.\n",
    "\n",
    "#### Actions\n",
    "* Four possible moves: up, down, left, right\n",
    "\n",
    "#### Rewards\n",
    "* Goal (G): +10.\n",
    "* Treasure (T): +5 per treasure.\n",
    "* Hole (H): -10.\n",
    "* Frozen tiles (F): 0.\n",
    "\n",
    "#### Environment\n",
    "Modify the FrozenLake environment in OpenAI Gym to include treasures (T) at certain positions. Inherit the original FrozenLakeEnv and modify the reset and step methods accordingly.\n",
    "Example grid:\n",
    "\n",
    "![image.png](attachment:image.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Outcomes:**\n",
    "1.\tCreate the custom environment by modifying the existing “FrozenLakeNotSlippery-v0” in OpenAI Gym and Implement the dynamic programming using value iteration and policy improvement to learn the optimal policy for the Treasure Hunt problem.\n",
    "2.\tCalculate the state-value function (V*) for each state on the map after learning the optimal policy.\n",
    "3.\tCompare the agent’s performance with and without treasures, discussing the trade-offs in reward maximization.\n",
    "4.\tVisualize the agent’s direction on the map using the learned policy.\n",
    "5.\tCalculate expected total reward over multiple episodes to evaluate performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required libraries and Define the custom environment - 2 Marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom environment to create the given grid and respective functions that are required for the problem\n",
    "\n",
    "#Include functions to take an action, get reward, to check if episode is over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration Algorithm - 1 Mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Improvement Function - 1 Mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the Optimal Value Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of the learned optimal policy - 1 Mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the policy - 1 Mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
